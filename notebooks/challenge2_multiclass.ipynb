{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Multi-class Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "1. Imports\n",
    "2. Functions\n",
    "3. Feature Extraction\n",
    "4. Exploratory Data Analysis (EDA)\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To hide deprication warnings \n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# Append helpers module to use the implemented modules from there\n",
    "import sys\n",
    "sys.path.append('../helpers/')\n",
    "\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import natsort\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern, hog\n",
    "\n",
    "# imported from helpers module\n",
    "from feature_selection import FeatureSelection\n",
    "from preprocessing import Preprocessing\n",
    "\n",
    "preprocessor    = Preprocessing()\n",
    "select_feature  = FeatureSelection()\n",
    "\n",
    "# PPrint object\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# To allow auto reload to this notebook after modifying any external file imported\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_objects(*arg):\n",
    "    '''Prints large and indented objects clearly.'''\n",
    "    pp.pprint(arg)\n",
    "\n",
    "def load_pickle(dir):\n",
    "    try:\n",
    "        with open(dir, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    except:\n",
    "        raise Exception(f'Error loading the pickle from {dir} directory.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _plot_histogram(self, hist, title='Histogram'):\n",
    "        # Determine the number of bins based on the length of the histogram\n",
    "        num_bins = len(hist)\n",
    "\n",
    "        # Create an array of bin edges\n",
    "        bin_edges = np.arange(num_bins)\n",
    "\n",
    "        # Create a bar plot for the histogram\n",
    "        plt.bar(bin_edges, hist)\n",
    "\n",
    "        # Set labels and a title\n",
    "        plt.xlabel('Bin')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(title)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_hist_features(self, hist):\n",
    "        feature_vector = []\n",
    "        \n",
    "        feature_vector.append(np.dot([i for i in range(0,256)], hist)) # Mean\n",
    "        feature_vector.append(np.argmax(hist)) # Mode\n",
    "        feature_vector.append(np.std(hist)) # Standard deviation\n",
    "        feature_vector.append(skew(hist)) # Skewness\n",
    "        feature_vector.append(np.sum(hist**2)) # Energy\n",
    "        feature_vector.append(entropy(hist, base=2)) # Entropy\n",
    "        feature_vector.append(kurtosis(hist)) # Kurtosis\n",
    "\n",
    "        return np.array(feature_vector)\n",
    "    \n",
    "    def _compute_norm_hist(self, image, bins=256, range=(0, 256), gray=False):\n",
    "        if not gray:\n",
    "            hist_ch1, _ = np.histogram(image[:, :, 0], bins=bins, range=range, density=True)\n",
    "            hist_ch2, _ = np.histogram(image[:, :, 1], bins=bins, range=range, density=True)\n",
    "            hist_ch3, _ = np.histogram(image[:, :, 2], bins=bins, range=range, density=True)\n",
    "\n",
    "            # self._plot_histogram(hist_ch1)\n",
    "            \n",
    "            # hist_ch1 = hist_ch1 / np.sum(hist_ch1)\n",
    "            # hist_ch2 = hist_ch2 / np.sum(hist_ch2)\n",
    "            # hist_ch3 = hist_ch3 / np.sum(hist_ch3)\n",
    "\n",
    "            # self._plot_histogram(hist_ch1)\n",
    "\n",
    "            return hist_ch1, hist_ch2, hist_ch3\n",
    "        else:\n",
    "            hist_gray, _ = np.histogram(image, bins=bins, range=range, density=True)\n",
    "            hist_gray = hist_gray / np.sum(hist_gray)\n",
    "\n",
    "            return hist_gray\n",
    "    \n",
    "    def extract_color_features(self, img):\n",
    "        # BGR\n",
    "        hist_b, hist_g, hist_r = self._compute_norm_hist(img)\n",
    "\n",
    "        # HSV\n",
    "        hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        hist_h, hist_s, hist_v = self._compute_norm_hist(hsv_img)\n",
    "\n",
    "        # LAB\n",
    "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        hist_L, hist_A, hist_B = self._compute_norm_hist(lab_img)\n",
    "\n",
    "        # Gray\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        hist_gray = self._compute_norm_hist(gray_img, gray=True)\n",
    "\n",
    "        color_features = np.concatenate((\n",
    "            self.calculate_hist_features(hist_r), \n",
    "            self.calculate_hist_features(hist_g), \n",
    "            self.calculate_hist_features(hist_b),\n",
    "            \n",
    "            self.calculate_hist_features(hist_h), \n",
    "            self.calculate_hist_features(hist_s), \n",
    "            self.calculate_hist_features(hist_v),\n",
    "\n",
    "            self.calculate_hist_features(hist_L),\n",
    "            self.calculate_hist_features(hist_A),\n",
    "            self.calculate_hist_features(hist_B),\n",
    "\n",
    "            self.calculate_hist_features(hist_gray),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return color_features\n",
    "    \n",
    "    def extract_glcm_features(self, image):    \n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise ValueError(\"Input must be a numpy array\")\n",
    "\n",
    "        if image.dtype != np.uint8:\n",
    "            # If the image is not 8-bit, convert it to 8-bit\n",
    "            # important as we set the levels for glcm to 256 for 8-bit images\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Compute GLCM matrix\n",
    "        distances = [1]\n",
    "        angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "        glcm = graycomatrix(gray_image, distances, angles, levels=256, symmetric=True, normed=True)\n",
    "        \n",
    "        # Extract texture features from GLCM matrix\n",
    "        features = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "        texture_features = np.concatenate([graycoprops(glcm, feature).ravel() for feature in features])\n",
    "        \n",
    "        return texture_features\n",
    "    \n",
    "    def extract_lbp_features(self, image):\n",
    "        ''' \n",
    "        Extracts the histogram of the LBP image for a certain number of neighbours and radius.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): the image from which the LBP features will be extracted (cropped ROI)\n",
    "            P (int): number of neighbours\n",
    "            R (int): radius\n",
    "        Output:\n",
    "            hist (numpy.array): histogram of the LBP image\n",
    "        '''\n",
    "\n",
    "        P=[8, 16] \n",
    "        R=[1, 2]\n",
    "        histogram = []\n",
    "\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        for idx, P_value in enumerate(P):\n",
    "\n",
    "            n_points = P_value * R[idx]\n",
    "\n",
    "            lbp_image = local_binary_pattern(gray_image, n_points, R[idx], method='uniform')\n",
    "\n",
    "            # Calculate the histogram of the LBP image\n",
    "            hist, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2), density=True)\n",
    "\n",
    "            # Normalize the histogram\n",
    "            hist /= np.sum(hist)\n",
    "\n",
    "            histogram = np.concatenate((histogram, hist))\n",
    "        \n",
    "        return histogram\n",
    "\n",
    "    def extract_hog_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract HOG features from an input image.\n",
    "\n",
    "        Parameters:\n",
    "        - image: Input image (grayscale).\n",
    "\n",
    "        Returns:\n",
    "        - hog_features: The extracted HOG features.\n",
    "        \"\"\"\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Define HOG parameters (you can customize these as needed)\n",
    "        pixels_per_cell = (8, 8)\n",
    "        cells_per_block = (2, 2)\n",
    "        orientations = 9\n",
    "\n",
    "        # Compute HOG features\n",
    "        hog_features = hog(gray_image, pixels_per_cell=pixels_per_cell,\n",
    "                           cells_per_block=cells_per_block,\n",
    "                           orientations=orientations)\n",
    "        \n",
    "        return hog_features\n",
    "    \n",
    "    def region_based_hist_fe(self, callback_func, image):\n",
    "        # We split the image into 3 parts, each is 1/3 of the original height\n",
    "        height, _, _ = image.shape\n",
    "        partial_height = height // 3\n",
    "\n",
    "        # Sotring the images into a list to iterate through them and get the feature vector for each part separately\n",
    "        images_list = [\n",
    "            image[0:partial_height, :],\n",
    "            image[partial_height:2*partial_height, :],\n",
    "            image[2*partial_height:, :],\n",
    "        ]\n",
    "\n",
    "        feature_vector = []\n",
    "\n",
    "        for img in images_list:\n",
    "            partial_feature_vector = callback_func(img)\n",
    "            feature_vector = np.concatenate((feature_vector, partial_feature_vector))\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "        \n",
    "    def fit(self, image):\n",
    "        # color_features      = self.extract_color_features(image)\n",
    "        color_features      = self.region_based_hist_fe(\n",
    "            callback_func=self.extract_color_features,\n",
    "            image=image\n",
    "        )\n",
    "        texture_features    = self.extract_glcm_features(image)\n",
    "        lbp_features        = self.extract_lbp_features(image)\n",
    "        # hog_features        = self.extract_hog_features(image) # 1 file of 7k samples ended up 24GB size\n",
    "\n",
    "        all_features = np.concatenate((color_features, texture_features, lbp_features))\n",
    "\n",
    "        return all_features\n",
    "    \n",
    "\n",
    "# img = r'../challenge1/train/nevus/nev00001.jpg'\n",
    "# img = cv2.imread(img)\n",
    "\n",
    "# # cv2.imshow(\"s\", img)\n",
    "# # cv2.waitKey(0)\n",
    "\n",
    "# fe = FeatureExtraction()\n",
    "# fv1 = fe.extract_color_features(img)\n",
    "# fv2 = fe.region_based_hist_fe(fe.extract_color_features, img)\n",
    "\n",
    "# # print(fv1)\n",
    "# print(fv2)\n",
    "\n",
    "\n",
    "# b,g,r = fe._compute_norm_hist(img)\n",
    "# print(np.min(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lui\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
